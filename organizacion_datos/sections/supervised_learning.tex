\documentclass[../main.tex]{subfiles}

\begin{document} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Supercised Learning}
    El Supercised Learning se puede clasificar en dos grandes grupos:
    \begin{enumerate}
        \item Classification:
        \begin{enumerate}
            \item Regresión logística.
            \item K-Nearest Neighbors (KNN).
            \item Decision Trees.
            \item Random Forest.
        \end{enumerate}
        \item Regression.
            \begin{enumerate}
                \item Linear Regression.
            \end{enumerate}
    \end{enumerate}

    
    Existen métodos y algoritmos que se pueden usar en ambos problemas:
    \begin{table}[ht]
        \begin{center}
            \begin{tabular}{|l|c|c|}
                \hline
                \textbf{Supervised Learning} & \textbf{Classification} & \textbf{Regression} \\
                \hline
                Logistic Regression & \checkmark &  \\ \hline
                k-Nearest Neighbors (KNN)  & \checkmark & \checkmark \\ \hline
                Decision Trees & \checkmark & \checkmark \\ \hline
                Random Forest & \checkmark & \checkmark \\ \hline
                
                Linear Regression &  & \checkmark \\
                \hline
            \end{tabular}
        \end{center}
        \caption{}
    \end{table}
    
    
    \paragraph{Métricas Clasificación:}
        \begin{itemize}
            \item Precisión.
            \item Recall (exhaustividad).
            \item Valor-F ($F_1$)
        \end{itemize}
        
        Ver video 03b min 40. Matriz de confucion.

    \subsection{Classification}
        En el contexto del aprendizaje automático supervisado, la variable predictora (también conocida como variable independiente o característica) es una variable que se utiliza para predecir el valor de la variable objetivo (también conocida como variable dependiente o target). Las variables predictoras son los datos de entrada al modelo y la variable objetivo es el resultado que se desea predecir.
        
        \subsubsection{Regresión logística.}
            La regresión logística es un tipo de algoritmo de aprendizaje supervisado que se utiliza principalmente para resolver tareas de “clasificación” binarias, aunque contiene la palabra "regresión". Se lo utiliza para predecir el resultado de una variable categórica (una variable que puede adoptar un número limitado de categorías) en función de las \textbf{variables predictoras} o independientes. Un ejemplo de clasificación podría ser la detección de spam: un programa de aprendizaje automático puede aprender a marcar el spam después de recibir ejemplos de correos electrónicos spam marcados (\textbf{variable objetivo} “target” sería una variable binaria que indica si un correo electrónico es spam o no) por los usuarios y ejemplos de correos electrónicos regulares no spam (también llamados “ham”). Notebook ejemplo \cite{regresión_logística_colab}.

        \subsubsection{K-Nearest Neighbors (KNN).}
            k vecinos más cercanos es un método de clasificación no paramétrico. Video de youtube \cite{knn_you_tube}.\\
            Se utiliza principalmente para la clasificación de datos no lineales y para resolver problemas de clasificación en los que los datos son muy complejos y desestructurados.\\
            
            Resumen:
            \begin{itemize}
                \item Es sensible a conjuntos de datos no balanceados.
                \item Es sensible aoutliers.
            \end{itemize}
    
        \subsubsection{Decision Trees}
            Existen varios algoritmos ID3, C4.5 y CART. Pagina buena \cite{tree_pag}:
        
            \paragraph{$\bullet$ ID3 - Iterative Dichotomiser 3:}
                Genera un árbol de decision a partir de un conjunto de ejemplos.\\
                Este algoritmo usa las metricas de \textit{entropía} y \textit{ganancia de la información}. Cuando se usa la librería sklearm \textit{sklearn.tree.DecisionTreeClassifier} en el parámetro \textit{criterion} colocar \textit{entropy}.
                
                \begin{itemize}
                    \item Un nodo principal llamado raíz en la parte superior.
                    \item Nodos terminales. como su nombre lo indica, son nodos donde termina el flujo y que ya no son raiz de ningún otro nodo. Estos nodos terminales deben contener una respuesta, o sea, la clasificación a que pertenece el objeto que ha conducido hasta él.
                    \item Los demás nodos representan preguntas con respecto al valor de uno de los atributos.
                    \item Las líneas nodos representan preguntas con respecto al valor de uno de sus atributos.
                    \item Las líneas representa las posibles respuestas que los atributos pueden tomar.
                \end{itemize}
        
                \textbf{Algoritmo básico}
                \begin{enumerate}
                    \item Calcular la entropía para todas las clases.
                    \item Calcular la entropía para cada valor posible de cada atributo.
                    \item Seleccionar el mejor atributo basado en la reducción de la entropía. usando el calculo de la ganancia de la información.
                    \item Iterar, para cada sub-nodo, Excluyendo el nodo raíz, que ya fue usado.
                \end{enumerate}
            
                \paragraph{$\bullet$ C4.5:}
                Utiliza la metrica \textit{gini}.
            
                \begin{enumerate}
                    \item Mitiga el sobreajuste por que emplea inherentemente el proceso de poda de un solo paso.
                    \item Funciona para datos discretos y continuos.
                    \item Es útil para datos incompletos.
                \end{enumerate}
            
            \underline{\textbf{Metricas:}}
            \begin{itemize} 
                \item 
                    \textbf{Entropía:} \\
                    La medida del desorden o la medida de la pureza. Básicamente, es la medida de la impureza o aleatoriedad de los datos.\\

                    Para calcular la entropia de $n$ clases se utiliza la fórmula:
                    \begin{equation}
                        H(S) = \sum_{i=1}^{n} -p_i \cdot  \log_2(p_i)
                    \end{equation}
                
                    Dónde:
                    \begin{itemize}
                        \item $S$: es una lista de valores posibles.
                        \item $p_i$: es la probabilidad de los valores.
                        \item $i$: cada uno de los valores.
                    \end{itemize}
                
                    Importante:
                    \begin{itemize}
                        \item Para una muestra homogénea la entropia es igual a cero 0. Si no existe aleatoriedad, es decir, una moneda cargada.
                        \item La máxima entropía viene dada por $\log_2(n)$, $n$ son los posibles valores de salida. Si $n=2$ (true o false) entonces, la máxima entropía es 1. O sea es la máxima incertidumbre, ejemplo moneda equilibrada.
                    \end{itemize}

                \item 
                    \textbf{Ganancia de la Información:}
                    La ganancia de la información se aplica a cuantificar qué característica, de un conjunto de datos dados, proporciona la máxima información sobre la clasificación.
            \end{itemize}               
        
        \subsubsection{Random Forest - Bosques aleatorios.}
            Usa una técnica, o meta-algoritmo llamado Bootstrap aggregating. Se crean m tablas reducidas en atributos y para cada una de ellas entrenamos un árbol.
            
    \subsection{Regresión}
        \subsubsection{Linear Regression}
            La regresión lineal es un método estadístico que se utiliza para estudiar la relación entre una variable dependiente (también conocida como variable objetivo) y una o más variables independientes (también conocidas como variables predictoras). El objetivo de la regresión lineal es encontrar la línea que mejor se ajuste a los datos y pueda utilizarse para hacer predicciones.
        
            En el caso de una sola variable independiente, la línea de regresión se puede representar mediante la ecuación $y = a + bx$, donde y es la variable dependiente, $x$ es la variable independiente, $a$ es la intersección en $y$ y $b$ es la pendiente de la línea. Los valores de $a$ y $b$ se determinan utilizando los datos disponibles para minimizar la suma de los errores cuadrados entre los valores observados y los valores predichos por la línea de regresión. Ver ejemplo %%%\cite{Ejemplo Regresión Lineal}
            
        \subsubsection{Regresión Lineal Múltiple}
            Consiste en predecir una respuesta numérica $y$ en base a múltiples variables predictoras $x_1, x_2, ..., x_n $, suponiendo una relación lineal. 
		
		\begin{equation}
			y = a_1 \cdot x_1 + a_2 \cdot x_2 + ... + a_n \cdot x_n + b
		\end{equation}
		
		\paragraph{$\bullet$ Métricas Regresión:}
                Las métricas de evaluación son medidas utilizadas para evaluar el rendimiento de un modelo de aprendizaje automático. En el caso de los problemas de regresión en el aprendizaje supervisado, estas métricas nos ayudan a determinar qué tan bien nuestro modelo está haciendo predicciones cuantitativas, como valores continuos.\\
		
    		\underline{Metricas para la regresión:}
    		\begin{itemize}
    			\item Raíz del error cuadrático medio \textbf{(RMSE)}.
    			\item Error absoluto medio \textbf{(MAE)}.
    			\item Error cuadratico medio \textbf{(MSE)}.
    			\item Suma Residual de los cuadrados \textbf{RSS}.
    		\end{itemize}
		
    		\underline{Para cada una de las métricas:}
    		\begin{itemize}
    			\item $m:$ número de instancias
    			\item $h:$ Funcion hipotesis, es el modelo entrenado. En este caso regresion lineal.
    			\item $x:$ Todos los valores de entrada, todas las columnas.
    		\end{itemize}
    		
    		\paragraph{Error cuadrático medio \textbf{(MSE)}:}
    		\begin{equation}
    			MSE(X, h) = \frac{1}{m} \cdot RSS = \frac{1}{m} \cdot \sum_{i=1}^{m} (h(x^{(i)}) - y^{(i)})^2
    		\end{equation}
    		
    		\paragraph{Error absoluto medio \textbf{(MAE)}:}
    		\begin{equation}
    			MAE(X, h) = \frac{1}{m} \cdot \sum_{i=1}^{m} |h(x^{(i)} - y^{(i)}|
    		\end{equation}
    		
    		\paragraph{Raíz del error cuadrático medio \textbf{(RMSE)}:}
    		\begin{equation}
    			RMSE(X, h) = \sqrt{\frac{1}{m} \cdot \sum_{i=1}^{m}(h(x^{(i)}-y^{(i)}))^2}
    		\end{equation}
		          
\end{document} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%