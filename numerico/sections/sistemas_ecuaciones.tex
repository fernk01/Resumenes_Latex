\documentclass[../main.tex]{subfiles}

\begin{document} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Sistemas de Ecuaciones Lineales y No Lineales} 

    \begin{definition} \textbf{(Matriz no singular)}.
        Una matriz cuyo determinante es no nulo.
        \begin{equation}
            det(A) \neq 0
        \end{equation}
    \end{definition}
        
    \begin{definition}\textbf{(Diagonal Dominante)}
        \begin{equation}
            |a_{ii}| \geq \sum_{j=1, j\neq i}^{n} |a_{ij}| \quad \quad para \quad i = 1,2,...,n
        \end{equation}
    \end{definition}

    \begin{definition}\textbf{(Matriz Simétrica)}
        \begin{equation}
            A = A^T
        \end{equation}
    \end{definition}

    \begin{definition}\textbf{(Matriz Definida Positiva)}
        \begin{equation}
            x^T \cdot A \cdot x > 0 \quad \forall x \neq 0
        \end{equation}
    \end{definition}


    \begin{definition}\textbf{(Norma infinito)} 
        Dada una matriz $A$ de $nxn$ dimensiones, se define la norma infinito de $A$ como:
        \begin{equation}
            ||A||_{\infty} = max_{1\leq i \leq n} \sum_{j=1}^{n} |a_{ij}|
        \end{equation}

        Ejemplo:
        \begin{equation}
            A = \begin{bmatrix*}[r]
                1 & 2 & -1 \\
                0 & 3 & -1 \\
                5 & -1 & 1
            \end{bmatrix*}
            \quad \quad
            ||A||_{\infty} = 7
        \end{equation}
        
        Paso a paso:
        \begin{equation}
            \begin{split}
                \sum_{j = i}^{3} &= |1| + |2| + |-1| = 4 \\
                \sum_{j = i}^{3} &= |0| + |3| + |-1| = 4 \\
                \sum_{j = i}^{3} &= |5| + |-1| + |1| = 7 \\
            \end{split}
        \end{equation}

    \end{definition}

    \subsection{Eliminación de Gauss y sustitución inversa}
        El Método de Eliminación de Gauss1 es un método directo muy efectivo que transforma una matriz cualquiera en una matriz triangular superior y luego aplica el método de sustitución inversa para obtener la solución del sistema dado. Para ello se basa en la propiedad que tienen las matrices de que la misma no cambia si se reemplaza alguna de sus las por una combinación lineal de ella con alguna de las restantes las. El procedimiento en líneas generales es:\\


        \subsubsection{Estrategias de pivoteo}
            Establecer como pivote un valor alto en módulo, para evitar el mal condicionamiento del algoritmo.
            \begin{itemize}
                \item \textbf{Pivoteo parcial:} 
                    Se intercambian las filas de la matriz para que el pivote sea el mayor en módulo.
                    
                    Paso $k$:
                    \begin{itemize}
                        \item Buscar $r$ tal que $|a_{rk}^{(k)}| = max|a_{ik}^{(k)}|$, $k \leq  i \leq n $
                        \item intercambiar filas $r$ y $k$
                    \end{itemize}

                    
                \item \textbf{Pivoteo total:} 
                    Se intercambian las filas y las columnas de la matriz para que el pivote sea el mayor en módulo.

                    Paso $k$:
                    \begin{itemize}
                        \item Buscar $r$ y $s$ tal que $|a_{rs}^{(k)}| = max|a_{ij}^{(k)}|$, $k \leq  i,j \leq n $
                        \item intercambiar filas $r$ y $k$ y columnas $s$ y $k$
                    \end{itemize}

            \end{itemize}

            Usando la estrategia de pivoteo se logra que el error de representación sea minimo (Video P03b min 3:25).
    \subsection{Factorización LU}
        \begin{teorema}{Teorema de la factorización LU}{}
            Sea una matriz cuadrada $(nxn)$ y suponga que A se puede reducir por renglones a
            una matriz triangular $U$ sin hacer alguna permutación entre sus renglones. Entonces
            existe una matriz triangular inferior $L$ invertible con unos en la diagonal tal que $A = LU$. Si, además, $U$ tiene $n$ pivotes (es decir, A es invertible), entonces esta factorización es única.
        \end{teorema}

        Descomposición o Factorización LU consiste en descomponer la matriz $\textbf{\textit{A}}$ original en el producto de dos matriz: una triangular inferior $(\textbf{\textit{L}})$ y una triangular superior $(\textbf{\textit{U}})$, para armar el siguiente sistema:

        \begin{equation}
            A \cdot x = B \quad \Rightarrow \quad L \cdot U \cdot x = B, \quad con \quad A = L \cdot U
        \end{equation}

        De esta forma obtenemos dos sistemas de ecuaciones:
        \begin{equation}
            \begin{split}
                L \cdot y &= B\\
                U \cdot x &= y
            \end{split}
        \end{equation}

        Para obnter las matriz $\textit{U}$ se triangula la matriz $\textit{A}$ mediante el método de eliminación de Gauss. Para obtener la matriz $\textit{L}$ se utiliza el mismo método, pero se guardan los \textit{multiplicadores} en la matriz $\textit{L}$ (Ver libro Peole pag 189).

        \begin{equation}
            %% Creamos una matriz de 3x3
            A = \begin{bmatrix*}[r]
                    2 & 1 & 3 \\
                    4 & -1 & 3 \\
                    -2 & 5 & 5 \\
                \end{bmatrix*}
        \end{equation}

        \begin{equation}
            %% Creamos una matriz de 3x3
            A = \begin{bmatrix*}[r]
                    2 & 1 & 3 \\
                    4 & -1 & 3 \\
                    -2 & 5 & 5 \\
                \end{bmatrix*}  \xrightarrow[]{\begin{subarray}{l}
                    R_2 - 2R_1\\
                    R_3 +  R_1\\
                \end{subarray}}
                \begin{bmatrix*}[r]
                    2 & 1 & 3 \\
                    0 & -3 & -3 \\
                    0 & 6 & 8 \\
                \end{bmatrix*}
                \xrightarrow{R_3 - 2R_2}
                \begin{bmatrix*}[r]
                    2 & 1 & 3 \\
                    0 & -3 & -3 \\
                    0 & 0 & 2 \\
                \end{bmatrix*} = U
        \end{equation}

        Con eso se obtine la matriz $\textit{U}$. Para obtener la matriz $\textit{L}$ se guarda los multiplicadores en la matriz $\textit{L}$.

        Los multiplicadores que se obtuvieron en la matriz $\textit{L}$ son:
        
        \begin{equation}
            \begin{split}
            R_2 - 2R_1 &\quad \Rightarrow \quad m_{21} = 2\\
            R_3 +  R_1 &\quad \Rightarrow \quad m_{31} = -1\\
            R_3 - 2R_2 &\quad \Rightarrow \quad m_{32} = -2
            \end{split}
        \end{equation}


        \begin{equation}
            %% Creamos una matriz de 3x3
            L = \begin{bmatrix*}[r]
                    1 & 0 & 0 \\
                    2 & 1 & 0 \\
                    -1 & -2 & 1 \\
            \end{bmatrix*}
        \end{equation}


        \subsubsection{La factorización PA = LU}
            \begin{teorema}
                Sea $A$ una matriz invertible de $n \times n$. Entonces existe una matriz de permutación $P$ tal que
                \begin{equation}
                    P \cdot A = L \cdot U
                \end{equation}
                donde $L$ es una matriz triangular inferior con unos en la diagonal y $U$ es una matriz triangular superior. La matriz $P$ es el producto de $n-1$ matrices de permutación elementales $P_{n-1} \cdot P_{n-2} \cdot \dots \cdot P_1$.   
            \end{teorema}
            
        
    \subsection{Condición de una matriz}
        Si el residuo es:
        \begin{equation}
            R = b - A \cdot \widetilde{x} = A \cdot x - A \cdot \widetilde{x} = A \cdot (x - \widetilde{x}) = A \cdot \delta x
        \end{equation}
        
        Es decir:
        \begin{equation}
            x - \widetilde{x} = A^{-1} \cdot R
        \end{equation}

        Aplicamos norma a ambos lados de la ecuación y usamos la desigualdad triangular:
        \begin{equation}
            ||x - \widetilde{x}|| \leq ||A^{-1}|| \cdot ||R||
            \label{eq:condicion_1}
        \end{equation}

        Hacemos lo mismo con $A \cdot  x = b$:
        \begin{equation}
            ||x|| \leq ||A^{-1}|| \cdot ||b||
            \label{eq:condicion_2}
        \end{equation}

        Dividimos miembro a miembro las ecuaciones (\ref{eq:condicion_1}) y (\ref{eq:condicion_2}):
        \begin{equation}
            \frac{||x - \widetilde{x}||}{||x||} \leq ||A^{-1}|| \cdot ||A|| \cdot \frac{||R||}{||b||}
        \end{equation}

        Definimos el \textbf{Número de Condición} de una matriz como:
        \begin{equation}
            \kappa(A) = ||A^{-1}|| \cdot ||A||
        \end{equation}

        El número de condición es $\kappa(A) \geq 1$. Si $\kappa(A) \approx 1$ la matriz es \textbf{bien condicionada}. Si $\kappa(A) \gg 1$ la matriz es \textbf{mal condicionada}.
    \subsection{Refinamiento Iterativo de la Solución}
        Si $A \cdot x = b$. Conocemos la solución $\widetilde{x}$ aproximada.\\
        \begin{enumerate}
            \item Calculamos el residuo con 
                \begin{equation}
                    R = b - A \cdot \widetilde{x}
                \end{equation}
                Se utiliza doble presición para calcular el residuo $2t$.
            
            \item Calcular el vertor de corrección $\delta x = x - \widetilde{x}$ con
                \begin{equation}
                    A \cdot \delta x = R
                \end{equation}
               
            \item Calcular la solución corregida $x = \widetilde{x} + \delta x$
            \end{enumerate}
    
    \subsection{Métodos}
        \begin{itemize}
            \item \textbf{Métodos Directos:} Se obtiene la solución en un número finito de pasos.
            
                Se suelen usar con matrices densas o casi llenas.
            \item \textbf{Métodos Iterativos:} Se obtiene la solución en un número infinito de pasos.
                \begin{itemize}
                    \item Método de Jacobi
                    \item Método de Gauss-Seidel
                    \item Método de SOR
                \end{itemize}

                Se suelen usar con matrices rala, con muchos ceros.
        \end{itemize}

    \subsection{Método de Jacobi}
        La forma tradicional:
        \begin{equation}
            x_{i}^{(k+1)} = \frac{b_i}{a_{ii}} - 
            \sum_{\begin{subarray}{l}
                        j= 1 \\
                        j \neq i
                    \end{subarray}
                }^{n} \frac{a_{ij}}{a_{ii}} \cdot x_j^{(k)}
        \end{equation}

        El método estacionario más sencillo es el Método de Jacobi. 
        \begin{equation}
            \begin{bmatrix*}[r]
                4 & 3 & 0 \\
                3 & 4 & -1 \\
                0 & -1 & 4 \\
            \end{bmatrix*}
            \cdot
            \begin{bmatrix*}[r]
                x_1 \\
                x_2 \\
                x_3 \\
            \end{bmatrix*}
            =
            \begin{bmatrix*}[r]
                24 \\
                30 \\
                -24 \\
            \end{bmatrix*}   
        \end{equation}
    
        Despejamos de la ecuación $x_1$, $x_2$ y $x_3$ en cualquier orden.
        \begin{equation}
            \begin{split}
                4 \cdot x_1 + 3 \cdot x_2 = 24 \quad \quad x_1 &= \frac{1}{4} \cdot (24 - 3 \cdot x_2)\\
                3 \cdot x_1 + 4 \cdot x_2 - x_3 = 30 \quad \quad x_2 &= \frac{1}{4} \cdot (30 + x_3 - 3 \cdot x_1)\\
                -x_2 + 4 \cdot x_3 = -24 \quad \quad x_3 &= \frac{1}{4} \cdot (-24 + x_2)\\
            \end{split}
            \label{eq:sist_despeje}
        \end{equation}

        Forma general:
        \begin{equation}
            \begin{split}
                x_1^{(k+1)} &= \frac{1}{4} \cdot (24 - 3 \cdot x_2^{(k)})\\
                x_2^{(k+1)} &= \frac{1}{4} \cdot (30 + x_3^{(k)} - 3 \cdot x_1^{(k)})\\
                x_3^{(k+1)} &= \frac{1}{4} \cdot (-24 + x_2^{(k)})\\
            \end{split}
        \end{equation}

        Colocamos la condición inicial para el vector $x$; se elige cualquier valor. Comumente se elige $x_1 = x_2 = x_3 = 0$.


    \subsection{Método de Gauss-Seidel}
        La forma tradicional:
        \begin{equation}
            x_{i}^{(k+1)} = \frac{b_i}{a_{ii}} - 
            \sum_{j= 1}^{i-1} \frac{a_{ij}}{a_{ii}} \cdot x_j^{(k+1)}
                -
            \sum_{j= i+1}^{n} \frac{a_{ij}}{a_{ii}} \cdot x_j^{(k)}
        \end{equation}

        Utilizando la ecuacion (\ref{eq:sist_despeje}), pero esta vez usando los valores de $x_1$, $x_2$ y $x_3$ de la iteración anterior.\\

        Forma general de la ecuacion (\ref{eq:sist_despeje}) es:
        \begin{equation}
            \begin{split}
                x_1^{(k+1)} &= \frac{1}{4} \cdot (24 - 3 \cdot x_2^{(k)})\\
                x_2^{(k+1)} &= \frac{1}{4} \cdot (30 - 3 \cdot x_1^{(k+1)} + x_3^{(k)})\\
                x_3^{(k+1)} &= \frac{1}{4} \cdot (-24 + x_2^{(k+1)})\\
            \end{split}
        \end{equation}


    \subsection{Método de SOR}
        La forma tradicional:
        \begin{equation}
            x_{i}^{(k+1)} = (1 - \omega) \cdot x_i^{(k)} + \frac{\omega}{a_{ii}} \cdot \left( b_i - \sum_{j= 1}^{i-1} a_{ij} \cdot x_j^{(k+1)} - \sum_{j= i+1}^{n} a_{ij} \cdot x_j^{(k)} \right)
        \end{equation}

        De Gauss-Seidel se obtine $x_i^{(k+1)}$ y el residuo $r_i^{(k)} = x_i^{(k+1)} - x_i^{(k)}$. Entonces la sobre-relajación de los residuos es:
        \begin{equation}
            x_{i}^{(k+1)} = x_i^{(k)} + \omega \cdot r_i^{(k)}
        \end{equation}

        Solo reemplazamos en el residuo $r_i^{(k)}$ el valor de $x_i^{(k+1)}$ de Gauss-Seidel, lo demás se queda como esta.\\

    \subsection{Criterios de interrupción}
        Podemos tomar como criterios para interrumpir las iteraciones, que $x-x^{(n)} < Tol$, siendo $Tol$ una valor definido arbitrariamente, generalmente relacionado con la precisión utilizada ($\mu$). Existen varios criterios que pueden aplicarse. Estos son:
        \begin{itemize}
            \item \textbf{Criterio de la norma infinito:}
                \begin{equation}
                    ||x^{(n)} - x||_{\infty} < Tol
                \end{equation}
            \item \textbf{Criterio de la norma 1:}
                \begin{equation}
                    ||x^{(n)} - x||_{1} < Tol
                \end{equation}
            \item \textbf{Criterio de la norma 2:}
                \begin{equation}
                    ||x^{(n)} - x||_{2} < Tol
                \end{equation}
            \item \textbf{Criterio de la norma relativa:}
                \begin{equation}
                    \frac{||x^{(n)} - x||_{\infty}}{||x^{(n)}||_{\infty}} < Tol
                \end{equation}
        \end{itemize}

    \subsection{Convergencia}
        El Método de Jacobi y Gauss-Seidel convergen rápidamente si la matriz $A$ es \textit{estrictamente diagonal dominante}, como se verá más adelante. En cambio, la convergencia es lenta si la matriz $A$ es cualquiera de las otras dos. Finalmente, si la matriz $A$ no cumple con ninguna de las definiciones anteriores, el método de Jacobi no converge.

        \begin{itemize}
            \item \textbf{Diagonal dominantencia:} 
                \begin{equation}
                    |a_{ii}| \geq \sum_{\begin{subarray}{l}
                        j= 1 \\
                        j \neq i
                    \end{subarray}
                }^{n} |a_{ij}|
                \end{equation}

            \item \textbf{Estrictamente Diagonal dominantencia:} 
                \begin{equation}
                    |a_{ii}| > \sum_{\begin{subarray}{l}
                        j= 1 \\
                        j \neq i
                    \end{subarray}
                }^{n} |a_{ij}|
                \end{equation}
         \end{itemize}

    \subsection{Resumen}


    \subsection{Ejercicios}
        \begin{example}
            Sea el siguiente sistema de ecuaciones:
            \begin{equation}
                \begin{split}
                    0.721 \cdot x_1 + 0.0352 \cdot x_2 &= 1.62\\
                    0.836 \cdot x_1 + 0.0410 \cdot x_2 &= 1.89\\
                \end{split}
            \end{equation}

            \begin{enumerate}
                \item Resolverlo utilizando la eliminación de Gauss con pivoteo. Hallar la Descomposición LU de la matriz de coeficientes.Trabajar con una precisión de 3 dígitos.
                \item Hallar dos refinamientos de la solución obtenida en el punto anterior, utilizando la descomposición LU.  
            \end{enumerate}

            \textbf{Solución:}
            \begin{enumerate}
                \item 
                    Se resuelve por eliminacion gaussiana utilizando estrategias de pivoteo parcial.

                    Se permutan las filas $F_1 \longleftrightarrow F_2$.
                    \begin{equation}
                        \begin{bmatrix*}[r]
                            0.836 & -0.410 & \vrule & 1.89\\
                            0.721 & -0.352 & \vrule & 1.62 
                        \end{bmatrix*}
                        \quad \quad m_{21} = \frac{a_{21}}{a_{11}} = \frac{0.721}{0.836} = 0.863
                    \end{equation}

                    Si ($F_2 - 0.863 \cdot F_1$) se obtiene:
                    \begin{equation}
                        \begin{bmatrix*}[r]
                            0.836 & -0.410 & \vrule & \hfill 1.89 \hfill\\
                            0 & 0.001 & \vrule & -9 \cdot 10^{-3} 
                        \end{bmatrix*}
                    \end{equation}

                    Entonces tenemos:
                    \begin{equation}
                        \left\lbrace
                        \begin{array}{ll}
                            x_2 = \frac{b_2}{a_{22}} = -9,00\\
                            x_1 = \frac{b_1 - a_{12} \cdot y}{a_{11}} = 2,15
                        \end{array}
                        \right.
                        \quad \Rightarrow \quad
                        \widetilde{x} = \begin{pmatrix*}[r]
                            -2.15\\
                            -9.00
                        \end{pmatrix*}
                    \end{equation}
        
                    Las matrices de la descomposición LU son:
                    \begin{equation}
                        L = \begin{bmatrix*}[r]
                            1 & 0\\
                            0.863 & 1
                        \end{bmatrix*}
                        \quad \quad
                        U = \begin{bmatrix*}[r]
                            0.836 & -0.410\\
                            0 & 0.001
                        \end{bmatrix*}
                    \end{equation}

                    Como se realizo una permitación de las filas $L \cdot U \neq A$. Entonces se define una matriz de permutación $P$ tal que $L \cdot U = P \cdot A$. En este caso la matriz de permutación es:
                    \begin{equation}
                        P = \begin{bmatrix*}[r]
                            0 & 1\\
                            1 & 0
                        \end{bmatrix*}
                    \end{equation}

                \item
                    Resolvemos el sistema:
                    \begin{equation}
                        \begin{split}
                            A \cdot \delta &= r\\
                            P \cdot A \cdot \delta &= P \cdot r\\
                            L \cdot U \cdot \delta &= P \cdot r\\
                            L \cdot \varepsilon &= P \cdot r\\
                        \end{split}
                    \end{equation}

                    \begin{equation}
                        \begin{split}
                            \left\lbrace
                            \begin{array}{ll}
                                L \cdot \varepsilon &= P \cdot r\\
                                U \cdot \delta &= \varepsilon
                            \end{array}
                            \right.
                        \end{split}
                    \end{equation}
                
                    \underline{Refinamiento 1:}\\
                    Con ($2t = 6$):
                    \begin{equation}
                        \begin{split}
                            r &= b - A \cdot \widetilde{x} \\   
                            r &= \begin{pmatrix*}[r]
                                1.62\\
                                1.89
                            \end{pmatrix*} - 
                            \begin{bmatrix*}[r]
                                0.836 & -0.410\\
                                0.721 & -0.352
                            \end{bmatrix*} \cdot
                            \begin{pmatrix*}[r]
                                -2.15\\
                                -9.00
                            \end{pmatrix*} \\
                            r &= \begin{pmatrix*}[r]
                                2.15 \cdot 10^{-3}\\
                                -2.60 \cdot 10^{-3}
                            \end{pmatrix*}
                        \end{split}
                    \end{equation}

                    Se resuelve el sistema:
                    \begin{equation}
                        \begin{split}
                            L \cdot \varepsilon &= P \cdot r \\
                            \begin{bmatrix*}[r]
                                1 & 0\\
                                0.862 & 1
                            \end{bmatrix*} \cdot
                            \begin{pmatrix*}[r]
                                \varepsilon_x\\
                                \varepsilon_y
                            \end{pmatrix*} &=
                            \begin{pmatrix*}[r]
                                -2.60 \cdot 10^{-3} \\
                                2.15 \cdot 10^{-3}
                            \end{pmatrix*}
                        \end{split}
                    \end{equation}
                    
                    \begin{equation}
                        \begin{split}
                            \left\lbrace
                            \begin{array}{ll}
                                \varepsilon_x &= -2.60 \cdot 10^{-3}\\
                                \varepsilon_y &= 4.39 \cdot 10^{-3}
                            \end{array}
                            \right.
                        \end{split}
                    \end{equation}
                    
                    \begin{equation}
                        \begin{split}
                            U \cdot \delta &= y \\
                            \begin{bmatrix*}[r]
                                0.836 & -0.410\\
                                0 & 0.001
                            \end{bmatrix*} \cdot
                            \begin{pmatrix*}[r]
                                \delta_x\\
                                \delta_y
                            \end{pmatrix*} &=
                            \begin{pmatrix*}[r]
                                -2.60 \cdot 10^{-3} \\
                                4.39 \cdot 10^{-3}
                            \end{pmatrix*}
                        \end{split}
                    \end{equation}

                    \begin{equation}
                        \begin{split}
                            \left\lbrace
                            \begin{array}{ll}
                                \delta_x &= 2.15\\
                                \delta_y &= 4.39
                            \end{array}
                            \right.
                        \end{split}
                    \end{equation}

                    El resultado del primer refinamiento es:
                    \begin{equation}
                        \begin{split}
                            \widetilde{x}_2 &= \widetilde{x} + \delta \\
                            \widetilde{x}_2 &= \begin{pmatrix*}[r]
                                -2.15\\
                                -9.00
                            \end{pmatrix*} + 
                            \begin{pmatrix*}[r]
                                2.15\\
                                4.39
                            \end{pmatrix*} \\
                            \widetilde{x}_2 &= \begin{pmatrix*}[r]
                                4.69 \cdot 10^{-3}\\
                                \hfill -4.61 \hfill
                            \end{pmatrix*}
                        \end{split}
                    \end{equation}

                    \underline{Refinamiento 2:}
                    \begin{equation}
                        \begin{split}
                            r_2 &= b - A \cdot \widetilde{x}_2 \\
                            r_2 &= \begin{pmatrix*}[r]
                                1.62\\
                                1.89
                            \end{pmatrix*} -
                            \begin{bmatrix*}[r]
                                0.721 & -0.352\\
                                0.836 & -0.410
                            \end{bmatrix*} \cdot
                            \begin{pmatrix*}[r]
                                4.69 \cdot 10^{-3}\\
                                \hfill -4.61 \hfill
                            \end{pmatrix*} \\
                            r_2 &= \begin{pmatrix*}[r]
                                -2.72 \cdot 10^{-3}\\
                                -0.10 \cdot 10^{-3}
                            \end{pmatrix*}
                        \end{split}
                    \end{equation}

                    \begin{equation}
                        \begin{split}
                            L \cdot \varepsilon_2  &= P \cdot r_2 \\
                            \begin{bmatrix*}[r]
                                1 & 0\\
                                0.862 & 1
                            \end{bmatrix*} \cdot
                            \begin{pmatrix*}[r]
                                \varepsilon_{2x}\\
                                \varepsilon_{2y}
                            \end{pmatrix*} &=
                            \begin{pmatrix*}[r]
                                -0.10 \cdot 10^{-3} \\
                                -2.72 \cdot 10^{-3}
                            \end{pmatrix*}
                        \end{split}
                    \end{equation}
                                    
            \end{enumerate}


            

        \end{example}





\end{document}  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
